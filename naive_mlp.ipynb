{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bowen/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/bowen/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# This notebook contains a naive mlp for garbage classification\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from vocab import Vocabulary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:10, 36419.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "EMB_DIM = 100 # embedding dimension to be used, choose from [50, 100, 200, 300]\n",
    "glove_path = os.path.expanduser('~/Downloads/glove.6B.{}d.txt'.format(EMB_DIM))\n",
    "embedding_dict = utils.load_glove_to_dict(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with 2169 items\n",
      "2169\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 10 # The maximum input sequence length, if longer, truncate; if shorter, pad\n",
    "\n",
    "complete_csv_path = './preprocessed.csv'\n",
    "complete_df = pd.read_csv(complete_csv_path)\n",
    "\n",
    "complete_vocab = utils.build_vocab(complete_df)\n",
    "\n",
    "wvecs = utils.build_wvecs(embedding_dict, complete_vocab)\n",
    "\n",
    "train_csv_path = './train.csv'\n",
    "val_csv_path = './val.csv'\n",
    "test_csv_path = './test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = utils.GarbageDataset(train_csv_path, complete_vocab, MAX_SEQ_LEN)\n",
    "val_dataset = utils.GarbageDataset(val_csv_path, complete_vocab, MAX_SEQ_LEN)\n",
    "test_dataset = utils.GarbageDataset(test_csv_path, complete_vocab, MAX_SEQ_LEN)\n",
    "\n",
    "# Construct dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=utils.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=utils.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline_mlp(nn.Module):\n",
    "    # Baseline mlp for garbage classification\n",
    "    def __init__(self, embedding_dim, vocab_size, max_seq_length):\n",
    "        super(baseline_mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(max_seq_length * 10, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.out = nn.Linear(10, 4)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        batch_size = sequence.shape[0]\n",
    "        embedded = self.embedding(sequence)\n",
    "        embedded = embedded.view(batch_size, -1)\n",
    "        z = self.fc1(embedded)\n",
    "        z = self.fc2(z)\n",
    "        z = self.out(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(loader: DataLoader, model):\n",
    "    \"\"\"Evaluate the classification accuracy of model on the given dataset\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): \n",
    "        model ([type])\n",
    "    \"\"\"\n",
    "    correct_counts = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input, target = batch\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            pred = model(input).squeeze(1)\n",
    "            pred_classes = torch.argmax(pred, dim=1)\n",
    "            correct_counts += torch.sum(pred_classes == target)\n",
    "    print(correct_counts.item() / len(loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "0.4300254452926209\n",
      "Epoch 0 loss 3964.3510863780975\n",
      "0.49872773536895676\n",
      "Epoch 1 loss 3738.705544233322\n",
      "0.5190839694656488\n",
      "Epoch 2 loss 3409.8607709407806\n",
      "0.5623409669211196\n",
      "Epoch 3 loss 2948.29946577549\n",
      "0.5877862595419847\n",
      "Epoch 4 loss 2465.821175098419\n",
      "0.5903307888040712\n",
      "Epoch 5 loss 2039.1027629375458\n",
      "0.6055979643765903\n",
      "Epoch 6 loss 1686.3782294988632\n",
      "0.6335877862595419\n",
      "Epoch 7 loss 1402.8048535585403\n",
      "0.6234096692111959\n",
      "Epoch 8 loss 1183.7822397351265\n",
      "0.6310432569974554\n",
      "Epoch 9 loss 1025.7813694775105\n",
      "0.6361323155216285\n",
      "Epoch 10 loss 984.9664301276207\n",
      "0.628498727735369\n",
      "Epoch 11 loss 1014.6579702347517\n",
      "0.628498727735369\n",
      "Epoch 12 loss 796.1268595904112\n",
      "0.6412213740458015\n",
      "Epoch 13 loss 622.7976109683514\n",
      "0.6234096692111959\n",
      "Epoch 14 loss 530.6231648027897\n",
      "0.6361323155216285\n",
      "Epoch 15 loss 423.4408654868603\n",
      "0.6335877862595419\n",
      "Epoch 16 loss 360.396874088794\n",
      "0.6412213740458015\n",
      "Epoch 17 loss 320.8222685456276\n",
      "0.6361323155216285\n",
      "Epoch 18 loss 288.8390903659165\n",
      "0.6361323155216285\n",
      "Epoch 19 loss 264.3269208855927\n",
      "0.6361323155216285\n",
      "Epoch 20 loss 245.89717072062194\n",
      "0.6310432569974554\n",
      "Epoch 21 loss 232.94077670760453\n",
      "0.6259541984732825\n",
      "Epoch 22 loss 224.50065735541284\n",
      "0.6310432569974554\n",
      "Epoch 23 loss 219.02166779339314\n",
      "0.6259541984732825\n",
      "Epoch 24 loss 215.23617520462722\n",
      "0.628498727735369\n",
      "Epoch 25 loss 213.1578649878502\n",
      "0.628498727735369\n",
      "Epoch 26 loss 214.9731260733679\n",
      "0.6259541984732825\n",
      "Epoch 27 loss 223.08574387896806\n",
      "0.6259541984732825\n",
      "Epoch 28 loss 236.37513764947653\n",
      "0.628498727735369\n",
      "Epoch 29 loss 253.23724433779716\n",
      "0.6183206106870229\n",
      "Epoch 30 loss 278.27824637293816\n",
      "0.6157760814249363\n",
      "Epoch 31 loss 328.22325606644154\n",
      "0.6361323155216285\n",
      "Epoch 32 loss 440.7062214501202\n",
      "0.6513994910941476\n",
      "Epoch 33 loss 554.1947563588619\n",
      "0.6513994910941476\n",
      "Epoch 34 loss 282.3484376622364\n",
      "0.6513994910941476\n",
      "Epoch 35 loss 208.54166976362467\n",
      "0.638676844783715\n",
      "Epoch 36 loss 184.9192961892113\n",
      "0.6437659033078881\n",
      "Epoch 37 loss 169.78700057975948\n",
      "0.6412213740458015\n",
      "Epoch 38 loss 161.98179507814348\n",
      "0.6412213740458015\n",
      "Epoch 39 loss 157.28544201888144\n",
      "0.6463104325699746\n",
      "Epoch 40 loss 156.89756073616445\n",
      "0.648854961832061\n",
      "Epoch 41 loss 158.9731755061075\n",
      "0.648854961832061\n",
      "Epoch 42 loss 162.90639070235193\n",
      "0.6513994910941476\n",
      "Epoch 43 loss 166.87536407075822\n",
      "0.6513994910941476\n",
      "Epoch 44 loss 170.30940810777247\n",
      "0.6463104325699746\n",
      "Epoch 45 loss 172.77112259948626\n",
      "0.6437659033078881\n",
      "Epoch 46 loss 175.5313597144559\n",
      "0.6412213740458015\n",
      "Epoch 47 loss 178.4304216695018\n",
      "0.6463104325699746\n",
      "Epoch 48 loss 180.73306327778846\n",
      "0.6463104325699746\n",
      "Epoch 49 loss 181.29798183357343\n",
      "0.6437659033078881\n",
      "Epoch 50 loss 179.9574358742684\n",
      "0.648854961832061\n",
      "Epoch 51 loss 177.97485695499927\n",
      "0.6463104325699746\n",
      "Epoch 52 loss 177.76091587357223\n",
      "0.6412213740458015\n",
      "Epoch 53 loss 178.40326310601085\n",
      "0.6412213740458015\n",
      "Epoch 54 loss 176.78015862451866\n",
      "0.6361323155216285\n",
      "Epoch 55 loss 173.69767003227025\n",
      "0.6437659033078881\n",
      "Epoch 56 loss 170.35484513547271\n",
      "0.6437659033078881\n",
      "Epoch 57 loss 166.89717740006745\n",
      "0.6437659033078881\n",
      "Epoch 58 loss 163.5191376595758\n",
      "0.6437659033078881\n",
      "Epoch 59 loss 160.8018419011496\n",
      "0.6437659033078881\n",
      "Epoch 60 loss 158.6154910628684\n",
      "0.6437659033078881\n",
      "Epoch 61 loss 156.87763860309497\n",
      "0.6437659033078881\n",
      "Epoch 62 loss 155.55107674514875\n",
      "0.6437659033078881\n",
      "Epoch 63 loss 154.611658281181\n",
      "0.6437659033078881\n",
      "Epoch 64 loss 153.97880383580923\n",
      "0.6463104325699746\n",
      "Epoch 65 loss 153.57187616126612\n",
      "0.6463104325699746\n",
      "Epoch 66 loss 153.32926012342796\n",
      "0.6463104325699746\n",
      "Epoch 67 loss 153.21191754564643\n",
      "0.6463104325699746\n",
      "Epoch 68 loss 153.18878295086324\n",
      "0.6437659033078881\n",
      "Epoch 69 loss 153.22450254345313\n",
      "0.6463104325699746\n",
      "Epoch 70 loss 153.28561255102977\n",
      "0.648854961832061\n",
      "Epoch 71 loss 153.34762970125303\n",
      "0.6539440203562341\n",
      "Epoch 72 loss 153.4056784650311\n",
      "0.6564885496183206\n",
      "Epoch 73 loss 153.4750594063662\n",
      "0.6539440203562341\n",
      "Epoch 74 loss 153.58499939506873\n",
      "0.6513994910941476\n",
      "Epoch 75 loss 153.77138013858348\n",
      "0.6539440203562341\n",
      "Epoch 76 loss 154.0701954262331\n",
      "0.6539440203562341\n",
      "Epoch 77 loss 154.51560396701097\n",
      "0.6539440203562341\n",
      "Epoch 78 loss 155.1405376517214\n",
      "0.6539440203562341\n",
      "Epoch 79 loss 155.9758126018569\n",
      "0.6539440203562341\n",
      "Epoch 80 loss 157.0479698786512\n",
      "0.6539440203562341\n",
      "Epoch 81 loss 158.37420299928635\n",
      "0.648854961832061\n",
      "Epoch 82 loss 159.96010056650266\n",
      "0.648854961832061\n",
      "Epoch 83 loss 161.79935077158734\n",
      "0.6412213740458015\n",
      "Epoch 84 loss 163.87831663060933\n",
      "0.638676844783715\n",
      "Epoch 85 loss 166.184031247627\n",
      "0.6412213740458015\n",
      "Epoch 86 loss 168.7106051640585\n",
      "0.6412213740458015\n",
      "Epoch 87 loss 171.45189852174371\n",
      "0.6412213740458015\n",
      "Epoch 88 loss 174.36304082535207\n",
      "0.6412213740458015\n",
      "Epoch 89 loss 177.27908930834383\n",
      "0.6412213740458015\n",
      "Epoch 90 loss 179.8178335763514\n",
      "0.6335877862595419\n",
      "Epoch 91 loss 181.37273779977113\n",
      "0.6361323155216285\n",
      "Epoch 92 loss 181.31729613430798\n",
      "0.6412213740458015\n",
      "Epoch 93 loss 179.3239830425009\n",
      "0.6335877862595419\n",
      "Epoch 94 loss 175.52477099280804\n",
      "0.6335877862595419\n",
      "Epoch 95 loss 170.45038114488125\n",
      "0.6412213740458015\n",
      "Epoch 96 loss 164.88096619397402\n",
      "0.638676844783715\n",
      "Epoch 97 loss 159.62703230325133\n",
      "0.638676844783715\n",
      "Epoch 98 loss 155.29980725049973\n",
      "0.6412213740458015\n",
      "Epoch 99 loss 152.17519457638264\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") \n",
    "\n",
    "mlp = baseline_mlp(100, len(complete_vocab)).to(device)\n",
    "mlp.embedding.weight.data.copy_(torch.from_numpy(np.array(wvecs)))\n",
    "max_epoch = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=5e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "\n",
    "print('Start training')\n",
    "for epoch in range(max_epoch):\n",
    "    mlp.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input, target = batch\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        pred = mlp(input).squeeze(1)\n",
    "        loss = loss_fn(pred, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * target.shape[0]\n",
    "    eval(val_loader, mlp)\n",
    "    print('Epoch {} loss {}'.format(epoch, epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6423173803526449\n"
     ]
    }
   ],
   "source": [
    "eval(test_loader, mlp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88599d39be6bb73bfcba134737126a1f34afe518a85e5e2c456db22b1d0276eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
