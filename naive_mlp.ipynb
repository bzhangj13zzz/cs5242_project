{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook contains a naive mlp for garbage classification\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from vocab import Vocabulary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:10, 36792.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_path = os.path.expanduser('~/Downloads/glove.6B.100d.txt')\n",
    "embedding_dict = utils.load_glove_to_dict(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with 2169 items\n",
      "2169\n"
     ]
    }
   ],
   "source": [
    "complete_csv_path = './preprocessed.csv'\n",
    "complete_df = pd.read_csv(complete_csv_path)\n",
    "\n",
    "complete_vocab = utils.build_vocab(complete_df)\n",
    "print(complete_vocab)\n",
    "\n",
    "wvecs = utils.build_wvecs(embedding_dict, complete_vocab)\n",
    "\n",
    "print(len(wvecs))\n",
    "\n",
    "train_csv_path = './train.csv'\n",
    "val_csv_path = './val.csv'\n",
    "test_csv_path = './test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = utils.GarbageDataset('./preprocessed.csv', complete_vocab)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline_mlp(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(baseline_mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 10, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.out = nn.Linear(10, 4)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        batch_size = sequence.shape[0]\n",
    "        embedded = self.embedding(sequence)\n",
    "        embedded = embedded.view(batch_size, -1)\n",
    "        z = self.fc1(embedded)\n",
    "        z = self.fc2(z)\n",
    "        z = self.out(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "7.508064516129032\n",
      "Epoch 0 loss 6612.10493535921\n",
      "3.056451612903226\n",
      "Epoch 1 loss 6127.971958398819\n",
      "6.838709677419355\n",
      "Epoch 2 loss 5439.284460067749\n",
      "5.830645161290323\n",
      "Epoch 3 loss 5258.055226325989\n",
      "6.556451612903226\n",
      "Epoch 4 loss 5117.64919757843\n",
      "6.75\n",
      "Epoch 5 loss 4960.16169834137\n",
      "6.57258064516129\n",
      "Epoch 6 loss 4763.5505294799805\n",
      "6.169354838709677\n",
      "Epoch 7 loss 4589.196720123291\n",
      "6.548387096774194\n",
      "Epoch 8 loss 4270.767127037048\n",
      "8.78225806451613\n",
      "Epoch 9 loss 4030.767740249634\n",
      "11.983870967741936\n",
      "Epoch 10 loss 3678.102199077606\n",
      "15.60483870967742\n",
      "Epoch 11 loss 3321.703118801117\n",
      "18.201612903225808\n",
      "Epoch 12 loss 2968.5224742889404\n",
      "19.975806451612904\n",
      "Epoch 13 loss 2628.9176198244095\n",
      "21.379032258064516\n",
      "Epoch 14 loss 2341.1288553476334\n",
      "22.443548387096776\n",
      "Epoch 15 loss 2095.168779551983\n",
      "23.475806451612904\n",
      "Epoch 16 loss 1886.1227180361748\n",
      "24.161290322580644\n",
      "Epoch 17 loss 1706.5889381170273\n",
      "24.846774193548388\n",
      "Epoch 18 loss 1550.9148988127708\n",
      "25.54032258064516\n",
      "Epoch 19 loss 1415.6518903076649\n",
      "26.20967741935484\n",
      "Epoch 20 loss 1297.5493351221085\n",
      "26.596774193548388\n",
      "Epoch 21 loss 1194.0868976712227\n",
      "27.016129032258064\n",
      "Epoch 22 loss 1103.4887617379427\n",
      "27.370967741935484\n",
      "Epoch 23 loss 1023.7676593959332\n",
      "27.637096774193548\n",
      "Epoch 24 loss 954.3687189221382\n",
      "27.846774193548388\n",
      "Epoch 25 loss 892.7013367414474\n",
      "28.0\n",
      "Epoch 26 loss 839.1120626404881\n",
      "28.169354838709676\n",
      "Epoch 27 loss 791.4981009960175\n",
      "28.362903225806452\n",
      "Epoch 28 loss 749.9679728746414\n",
      "28.508064516129032\n",
      "Epoch 29 loss 713.0521864965558\n",
      "28.653225806451612\n",
      "Epoch 30 loss 680.700659006834\n",
      "28.733870967741936\n",
      "Epoch 31 loss 651.7171597108245\n",
      "28.782258064516128\n",
      "Epoch 32 loss 626.0929425172508\n",
      "28.887096774193548\n",
      "Epoch 33 loss 602.7203014232218\n",
      "28.93548387096774\n",
      "Epoch 34 loss 581.9434065669775\n",
      "28.95967741935484\n",
      "Epoch 35 loss 563.3499813638628\n",
      "28.967741935483872\n",
      "Epoch 36 loss 545.7524141781032\n",
      "28.975806451612904\n",
      "Epoch 37 loss 530.8879583273083\n",
      "29.0\n",
      "Epoch 38 loss 516.0569345746189\n",
      "29.04032258064516\n",
      "Epoch 39 loss 502.77079276926816\n",
      "29.04032258064516\n",
      "Epoch 40 loss 490.45008600503206\n",
      "29.120967741935484\n",
      "Epoch 41 loss 481.12885595858097\n",
      "29.120967741935484\n",
      "Epoch 42 loss 469.41561366431415\n",
      "29.112903225806452\n",
      "Epoch 43 loss 460.67425028048456\n",
      "29.056451612903224\n",
      "Epoch 44 loss 451.724660734646\n",
      "29.008064516129032\n",
      "Epoch 45 loss 445.703719609417\n",
      "28.983870967741936\n",
      "Epoch 46 loss 437.8070160187781\n",
      "28.983870967741936\n",
      "Epoch 47 loss 432.0541427442804\n",
      "28.983870967741936\n",
      "Epoch 48 loss 423.1438454259187\n",
      "28.951612903225808\n",
      "Epoch 49 loss 415.80561520159245\n",
      "28.903225806451612\n",
      "Epoch 50 loss 408.1860605296679\n",
      "28.927419354838708\n",
      "Epoch 51 loss 403.64278271701187\n",
      "28.89516129032258\n",
      "Epoch 52 loss 398.64683006424457\n",
      "28.879032258064516\n",
      "Epoch 53 loss 394.4975423924625\n",
      "28.89516129032258\n",
      "Epoch 54 loss 389.39046310959384\n",
      "28.870967741935484\n",
      "Epoch 55 loss 383.86010263487697\n",
      "28.887096774193548\n",
      "Epoch 56 loss 380.58680116245523\n",
      "28.911290322580644\n",
      "Epoch 57 loss 376.09137429483235\n",
      "28.943548387096776\n",
      "Epoch 58 loss 372.282767967321\n",
      "28.919354838709676\n",
      "Epoch 59 loss 368.72844432480633\n",
      "28.975806451612904\n",
      "Epoch 60 loss 365.25922882929444\n",
      "29.06451612903226\n",
      "Epoch 61 loss 361.496090145316\n",
      "29.06451612903226\n",
      "Epoch 62 loss 358.2520340802148\n",
      "29.14516129032258\n",
      "Epoch 63 loss 354.5644884281792\n",
      "29.169354838709676\n",
      "Epoch 64 loss 350.9737790413201\n",
      "29.20967741935484\n",
      "Epoch 65 loss 346.835918599274\n",
      "29.20967741935484\n",
      "Epoch 66 loss 343.132286017295\n",
      "29.25\n",
      "Epoch 67 loss 342.14714534953237\n",
      "29.25\n",
      "Epoch 68 loss 340.64201184920967\n",
      "29.298387096774192\n",
      "Epoch 69 loss 337.68109274003655\n",
      "29.330645161290324\n",
      "Epoch 70 loss 334.4911023476161\n",
      "29.419354838709676\n",
      "Epoch 71 loss 331.25423350697383\n",
      "29.419354838709676\n",
      "Epoch 72 loss 328.3002237859182\n",
      "29.491935483870968\n",
      "Epoch 73 loss 325.68288333155215\n",
      "29.5\n",
      "Epoch 74 loss 322.4119052309543\n",
      "29.56451612903226\n",
      "Epoch 75 loss 317.2340142976027\n",
      "29.56451612903226\n",
      "Epoch 76 loss 317.27447138074785\n",
      "29.612903225806452\n",
      "Epoch 77 loss 315.90498070186004\n",
      "29.612903225806452\n",
      "Epoch 78 loss 314.06049071485177\n",
      "29.70967741935484\n",
      "Epoch 79 loss 312.11269146599807\n",
      "29.68548387096774\n",
      "Epoch 80 loss 309.23596718674526\n",
      "29.725806451612904\n",
      "Epoch 81 loss 307.0061397245154\n",
      "29.717741935483872\n",
      "Epoch 82 loss 308.3357907203026\n",
      "29.782258064516128\n",
      "Epoch 83 loss 304.0919568866957\n",
      "29.782258064516128\n",
      "Epoch 84 loss 301.24779233476147\n",
      "29.862903225806452\n",
      "Epoch 85 loss 300.12968318001367\n",
      "29.830645161290324\n",
      "Epoch 86 loss 298.2003913363442\n",
      "29.89516129032258\n",
      "Epoch 87 loss 301.8792188435327\n",
      "29.967741935483872\n",
      "Epoch 88 loss 296.16097485949285\n",
      "29.951612903225808\n",
      "Epoch 89 loss 296.4249646780081\n",
      "30.024193548387096\n",
      "Epoch 90 loss 291.47398407477885\n",
      "30.04032258064516\n",
      "Epoch 91 loss 291.19438742054626\n",
      "30.024193548387096\n",
      "Epoch 92 loss 287.8816511211917\n",
      "30.072580645161292\n",
      "Epoch 93 loss 289.54914726363495\n",
      "30.120967741935484\n",
      "Epoch 94 loss 283.54830507421866\n",
      "30.096774193548388\n",
      "Epoch 95 loss 286.8224532841705\n",
      "30.161290322580644\n",
      "Epoch 96 loss 283.37037081993185\n",
      "30.161290322580644\n",
      "Epoch 97 loss 283.54251437075436\n",
      "30.201612903225808\n",
      "Epoch 98 loss 279.07854323578067\n",
      "30.20967741935484\n",
      "Epoch 99 loss 278.889177902136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") \n",
    "\n",
    "mlp = baseline_mlp(100, len(complete_vocab)).to(device)\n",
    "mlp.embedding.weight.data.copy_(torch.from_numpy(np.array(wvecs)))\n",
    "max_epoch = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=5e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "\n",
    "def eval(loader, model):\n",
    "    mean_acc = 0\n",
    "    correct_counts = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input, target = batch\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            pred = mlp(input).squeeze(1)\n",
    "            pred_classes = torch.argmax(pred, dim=1)\n",
    "            correct_counts += torch.sum(pred_classes == target)\n",
    "    print(correct_counts.item() / len(loader))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Start training')\n",
    "for epoch in range(max_epoch):\n",
    "    mlp.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input, target = batch\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        pred = mlp(input).squeeze(1)\n",
    "        loss = loss_fn(pred, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * target.shape[0]\n",
    "    eval(loader, mlp)\n",
    "    print('Epoch {} loss {}'.format(epoch, epoch_loss))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88599d39be6bb73bfcba134737126a1f34afe518a85e5e2c456db22b1d0276eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
